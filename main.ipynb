{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.ml.feature import Tokenizer,HashingTF,IDF,CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"python3.6\"\n",
    "sc = pyspark.SparkContext('local[10]')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mullah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mullah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mullah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "def build_tf_idf_model(data,vocab_size=82700):\n",
    "    sentenceData = spark.createDataFrame(data, [\"label\", \"words\"])\n",
    "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=vocab_size)\n",
    "    featurizedData = hashingTF.transform(sentenceData)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "    final_data = rescaledData.select(\"features\")\n",
    "    final_data_label = rescaledData.select(\"label\")\n",
    "    final_data1 = final_data.rdd.map(tuple)\n",
    "    m = final_data1.collect()\n",
    "    sparse_vector_col = [i[0] for i in m]\n",
    "    matrix = np.concatenate([x.toArray().reshape(1,-1) for x in sparse_vector_col])\n",
    "    sum_norm = np.sqrt(np.array([np.sum(np.power(i,2)) for i in matrix]).reshape(-1,1))\n",
    "    return idfModel,matrix,sum_norm,hashingTF\n",
    "def get_preprocessed_data(): \n",
    "    df = pd.read_csv('hyperlink.txt',sep='.txt ',header=None,engine='python')\n",
    "    links = list(df.iloc[:,1])\n",
    "    filenames = list(df.iloc[:,0])\n",
    "    data_col = ['./data_e/data/'+str(a)+'.txt' for a in filenames]\n",
    "    word_col = []\n",
    "    puncs = set(list(string.punctuation))\n",
    "    stop_words = set(stopwords.words('english')).union(puncs)\n",
    "    stemmer = PorterStemmer()\n",
    "    vocab = []\n",
    "    for i in range(len(data_col)):\n",
    "        with open(data_col[i],'r') as f:\n",
    "            words = ' '.join(f.readlines())\n",
    "            words = list(re.findall(\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\",words))\n",
    "            vocab.append(\" \".join(words))\n",
    "            words = [stemmer.stem(a.lower()) for a in words if a.lower() not in stop_words]\n",
    "            word_col.append(words)\n",
    "            \n",
    "    data = list(zip(links,word_col))\n",
    "    return data,vocab\n",
    "def process_query(sentence):\n",
    "    stemmer = PorterStemmer()\n",
    "    puncs = set(list(string.punctuation))\n",
    "    stop_words = set(stopwords.words('english')).union(puncs)\n",
    "    words = list(re.findall(\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\",sentence))\n",
    "    words = [stemmer.stem(a.lower()) for a in words if a.lower() not in stop_words]\n",
    "    return [('query',words)]\n",
    "def get_query_tf_idf(idfmodel,hashingTF,query):\n",
    "    sentenceData = spark.createDataFrame(query, [\"label\", \"words\"])\n",
    "    featurizedData = hashingTF.transform(sentenceData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "    final_data = rescaledData.select(\"label\", \"features\")\n",
    "    return final_data.rdd.map(tuple).collect()[0][1]\n",
    "def return_query(idfModel,hashingTF,query,matrix,mat_norm,printable_data):\n",
    "    start_time = time.time()\n",
    "    query_vector = get_query_tf_idf(idfModel,hashingTF,process_query(query))\n",
    "    dense_query = np.array(Vectors.dense(query_vector)).reshape(-1,1)\n",
    "    query_norm = np.sqrt(np.sum(np.power(dense_query,2)))\n",
    "    dot_collection = np.matmul(matrix,dense_query)\n",
    "    all_norm = mat_norm*query_norm+1e-20\n",
    "    cosine_simillarities = np.divide(dot_collection,all_norm).reshape(-1)\n",
    "    tup = zip(cosine_simillarities, range(len(cosine_simillarities)))\n",
    "    indexes = sorted(tup, key=lambda v: v[0], reverse=True)\n",
    "    from termcolor import colored, cprint \n",
    "    end_time = time.time()\n",
    "    text = colored(\"Showing 5 top results in \"+str(end_time-start_time)+' seconds', 'blue', attrs=['reverse', 'blink'])\n",
    "    print(text)\n",
    "    for i in indexes[:5]:\n",
    "        print('-'*90)\n",
    "        text = colored(data[i[1]][0], 'red', attrs=['reverse', 'blink'])\n",
    "        print(text)\n",
    "        text = colored(printable_data[i[1]][:500], 'green', attrs=['reverse', 'blink'])\n",
    "        print(text)\n",
    "        print('-'*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the data\n",
      "Preprocessing for 12000 docs took 115.27342653274536 seconds\n",
      "Building the TfIdf Model with vocabulary size = 83700\n",
      "Building the tfidf model took 54.45584774017334 seconds\n",
      "Write your query. (If you want to exit write exit)\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(\"Preprocessing the data\")\n",
    "start_time = time.time()\n",
    "data,prinatable_data = get_preprocessed_data()\n",
    "end_time = time.time()\n",
    "print('Preprocessing for',len(data),'docs took',end_time-start_time,'seconds')\n",
    "print(\"Building the TfIdf Model with vocabulary size = 83700\")\n",
    "start_time = time.time()\n",
    "idfModel,matrix,mat_norm,hashingTF = build_tf_idf_model(data)\n",
    "end_time = time.time()\n",
    "print('Building the tfidf model took',end_time-start_time,'seconds')\n",
    "while True:\n",
    "    query = input(\"Write your query. (If you want to exit write exit)\\n\")\n",
    "    if query in ['exit']:\n",
    "        break\n",
    "    else:\n",
    "        return_query(idfModel,hashingTF,query,matrix,mat_norm,prinatable_data)\n",
    "        input_2 = input(\"Write 1 to do another query and 0 to exit\")\n",
    "        if int(input_2)==0:\n",
    "            break\n",
    "        else:\n",
    "            clear_output()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
